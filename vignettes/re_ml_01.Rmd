---
title: "Report_5"
author: "Dario Maccioni"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: yes
    toc_float: yes
    numbered: yes
    theme: journal
---

# AGDS Report

## Report Exercise 5

### Task 1 - Data Visualization

```{r message = FALSE}
half_hourly_fluxes <- readr::read_csv("https://raw.githubusercontent.com/geco-bern/agds/main/data/df_for_stepwise_regression.csv") # Loading the dataset (.csv) into R and creating a table
```

```{r}
visdat::vis_miss(half_hourly_fluxes) # Checking if any data in the dataset is missing
```

```{r}
half_hourly_fluxes_fixed <- stats::na.omit(half_hourly_fluxes) # Deleting all rows with at least one value which equals "NA"
```

```{r message = FALSE, warning=FALSE}
library(ggplot2)
# Data cleaning: looks ok, no obviously bad data
# no long tail, therefore no further target engineering
half_hourly_fluxes |> 
  ggplot(aes(x = GPP_NT_VUT_REF, y = ..count..)) + 
  geom_histogram()
```


### Task 2 - Creating a linear regression model and a KNN model

```{r message = FALSE, warning=FALSE}
library(rsample)
library(tidyr)
library(caret)
library(recipes)
# Data splitting
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(half_hourly_fluxes_fixed, prop = 0.7, strata = "VPD_F")
half_hourly_fluxes_train <- rsample::training(split)
half_hourly_fluxes_test <- rsample::testing(split)

# Model and pre-processing formulation, use all variables but LW_IN_F
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = half_hourly_fluxes_train |> drop_na()) |> 
  recipes::step_BoxCox(all_predictors()) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())

# Fit linear regression model
mod_lm <- caret::train(
  pp, 
  data = half_hourly_fluxes_train |> drop_na(), 
  method = "lm",
  trControl = caret::trainControl(method = "none"),
  metric = "RMSE"
)

# Fit KNN model
mod_knn <- caret::train(
  pp, 
  data = half_hourly_fluxes_train, 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = data.frame(k = 8),
  metric = "RMSE"
)
```

### Task 3 - Loading the Function created on a .R file
```{r message = FALSE, warning=FALSE}
source("../Functions/large_file.R")
```


### Task 4 - Visualising the 2 models with the function created before

```{r warning=FALSE, message=FALSE}

# Linear Regression
eval_model(mod = mod_lm, df_train = half_hourly_fluxes_train, df_test = half_hourly_fluxes_test)

```

```{r warning=FALSE, message=FALSE}
# KNN
eval_model(mod = mod_knn, df_train = half_hourly_fluxes_train, df_test = half_hourly_fluxes_test)
```

### Task 5 - Visualising observed and modelled GPP for KNN and Linear Regressio

```{r}
library(ggplot2)

# create a dataframe with the dates and observed GPP
observed <- data.frame(Date = half_hourly_fluxes_fixed$TIMESTAMP, GPP = half_hourly_fluxes_fixed$GPP_NT_VUT_REF)

# add columns with modelled GPP for linear regression and KNN
observed$lm <- predict(mod_lm, newdata = half_hourly_fluxes_fixed)
observed$knn <- predict(mod_knn, newdata = half_hourly_fluxes_fixed)

# create a line plot for observed and modelled GPP for linear regression
ggplot(observed, aes(x = Date)) +
  geom_line(aes(y = GPP, color = "Observed")) +
  geom_line(aes(y = lm, color = "Linear Regression")) +
  labs(x = "Date", y = "GPP", color = "Model") +
  ggtitle("Observed and Modelled GPP for Linear Regression")

# create a line plot for observed and modelled GPP for KNN
ggplot(observed, aes(x = Date)) +
  geom_line(aes(y = GPP, color = "Observed")) +
  geom_line(aes(y = knn, color = "KNN")) +
  labs(x = "Date", y = "GPP", color = "Model") +
  ggtitle("Observed and Modelled GPP for KNN")

```

### Task 6 - the role of k

#### Task 6.1 - creating a hypothesis

Based on the understanding of KNN i got of this lecture, i would suppose that if the k-value approaches to 1, the model has a lower variance but a higher bias, which results in a lower R2 value on the test and training models. The reason why this is the case, is that if k=1, the model will only use a single neighbor. Therefore the model is just dependent of this neighbor (lower variance because just focusing on one variable). This leads to overfitting of the training model which means it will not generalize well to new data. Therefore the R2 of the test model will decrease.

If the k-value approaches to N, i suppose the model will have a higher variance and a lower bias, because it focuses on all variable which are available. Therefore the model will generalize well to new data. Also the R2 value increases on the test model.

For the information i got from the MAE i suppose, that if the k-value approaches to 1, the MAE will be higher, because the model overfits. If the k-value approaches to N, the MAE will be lower on the test model, because the model adapts well to new data.

Overall i think it is difficult to find the optimal k-value, because it depends on the dataset but also on the model created.

#### Task 6.2 - testing the hypothesis

#### function of the code

```{r}
library(rsample)
library(tidyr)
library(caret)
library(recipes)
library(ggplot2)

# Evaluate KNN model for different values of k
k_values <- c(1:30)
mae_values <- sapply(k_values, function(k) get_mae(k)$mae_test)
mae_train_values <- sapply(k_values, function(k) get_mae(k)$mae_train)

# Visualize MAE as a function of k
df <- data.frame(k = k_values, MAE_test = mae_values, MAE_train = mae_train_values)
ggplot(df, aes(x = k)) + 
  geom_line(aes(y = MAE_test, color = "Test")) + 
  geom_line(aes(y = MAE_train, color = "Train")) +
  geom_point(aes(y = MAE_test)) + 
  geom_point(aes(y = MAE_train)) + 
  xlab("k") + 
  ylab("MAE") +
  scale_color_manual(name = "Data set", values = c("red", "blue"))
```

#### Finding the opitmal k-value

```{r warning=FALSE}
# Find the optimal k
optimal_k <- k_values[which.min(mae_values)]

# Print optimal k value and MAE on test and training sets
cat("Optimal k value:", optimal_k, "\n")
cat("MAE on test set for optimal k value:", get_mae(optimal_k)$mae_test, "\n")
cat("MAE on training set for optimal k value:", get_mae(optimal_k)$mae_train, "\n")
```

