---
title: "re_ml_01"
author: "Dario Maccioni"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: yes
    toc_float: yes
    numbered: yes
    theme: journal
---

# AGDS Report Exercise 5

This Report Exercise implements a comparison of linear Regression and KNN models. Both were evaluated for a training set and a test set. The goal of this Report is to understand the observed differences between these two models. The Report is structured in 4 different Tasks with subchapters. I also created a extra .R file named ml_01_func.R which contains large functions, which would influence the reading flow and the length of this Report.

## Task 1 - Data Visualization

```{r message = FALSE}
half_hourly_fluxes <- readr::read_csv("../data/df_for_stepwise_regression.csv") # Loading the dataset (.csv) into R and creating a table
```

As shown down below, approximately 5.7% of the whole data set are missing values. If one looks at the different predictors one sees that as a example the LW_IN_F_MDS Predictor has nearly 50% missing values. This is important to notice for further analysis. 

```{r}
visdat::vis_miss(half_hourly_fluxes) # Checking if any data in the dataset is missing
```

```{r}
half_hourly_fluxes_fixed <- stats::na.omit(half_hourly_fluxes) # Deleting all rows with at least one value which equals "NA"
```

This chart shows if there is any "bad data" in the data frame. As it can be seen, there is no long tail or gaps or outliers and the data looks ok. There is no obvious "bad data". 

```{r message = FALSE, warning=FALSE}
# Plotting the data to see if there is bad data
library(ggplot2)
half_hourly_fluxes |> 
  ggplot(aes(x = GPP_NT_VUT_REF, y = ..count..)) + 
  geom_histogram()
```

## Task 2 - Creating and visualising a linear regression model and a KNN model

```{r message = FALSE, warning=FALSE}
library(rsample)
library(tidyr)
library(caret)
library(recipes)

# splitting the data in a training (70%) and test set (30%)
set.seed(2000) # just for reproducibility
split <- rsample::initial_split(half_hourly_fluxes_fixed, prop = 0.7, strata = "VPD_F")
half_hourly_fluxes_train <- rsample::training(split)
half_hourly_fluxes_test <- rsample::testing(split)

# pre_processing and ignoring the LW_IN_F variable because it has to much missing value and its probably not relevant for predicting GPP
pre_model <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = half_hourly_fluxes_train |> drop_na()) |> 
  recipes::step_BoxCox(all_predictors()) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())

# making a linear Regression model
model_linear_regression <- caret::train(
  pre_model, 
  data = half_hourly_fluxes_train |> drop_na(), 
  method = "lm",
  trControl = caret::trainControl(method = "none"),
  metric = "RMSE"
)

# making a KNN model
model_knn <- caret::train(
  pre_model, 
  data = half_hourly_fluxes_train, 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = data.frame(k = 8),
  metric = "RMSE"
)
```

### Task 2.1 - Loading the Function created on a .R file

A folder named "Functions" was created to put large codes in it to simplify coding and the readflow. The code can be extract by the function source().

```{r message = FALSE, warning=FALSE}
source("../Functions/ml_01_func.R") # loading the function created for Task 2 and Task 4. File contains two different functions.
```

### Task 2.2 - Visualising the 2 models with the function created before

__Linear Regression__ 

```{r warning=FALSE, message=FALSE}
# plotting the linear regression model out of the function created (saved in the Functions folder)
eval_model(mod = model_linear_regression, df_train = half_hourly_fluxes_train, df_test = half_hourly_fluxes_test)
```

__KNN__

```{r warning=FALSE, message=FALSE}
# Plotting the KNN out of the function created (saved in the Functions folder)
eval_model(mod = model_knn, df_train = half_hourly_fluxes_train, df_test = half_hourly_fluxes_test)
```

### 2.3 - Interpretation of the observed differences in the context of the bias-variance trade-off

__Why is the difference between the evaluation on the training and the test set larger for the KNN model than for the linear regression model?__

If one analyses the RMSE for between training and test set for both models, it can be seen, that the difference on the KNN is higher than the linear regression model. In this case it could be, because the KNN is more prone to overfitting than the linear regression model. Therefore the model makes accurate predictions for the training set (means low RMSE). However the overfitting could lead to a problem when facing a new data set (test set) because the model is to much specilized on the training set data (resulting in higher RMSE for test set). On the other site the difference between training and test set on the linear regression model is lower than KNN. This could be because the linear regression model makes linearity assumptions and therfore fits for training and test set. This makes it less flexible than the KNN model but more consistent for perfomance between training and test set.

__Why does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model?__

The evaluation on the test set could indicate a better model performance of the KNN model than the linear regression model because the KNN is better able to capture the underlying structure of the data and looks beyond the linear relationship between predictor and dependent variable, as the linear regression model does. In this case the KNN may generalize better to new data than the linear regression modeel. It is also important to choose a good number of neighbors because a poor number could influence the model performance.

__How would you position the KNN and the linear regression model along the spectrum of the bias-variance trade-off?__

It is very difficult to position these models in this spectrum, because both have their benefits and trade-offs. The KNN model has a higher variance (e.g. Illustration Task 3) and a lower bias, whereas the linear regression model comes with a lower variance (e.g. Illustration Task 3) and a high bias. The low bias of the KNN models is because they do not make assumptions about the relationship between the predictors and the dependent variable. The high variance makes the KNN model more vulnerable to overfitting. On the other hand the linear regression model make assumption about the linear relationship between predictors and dependent variable which increases the bias and decreases the variance. This leads to a simpler model which is less vulnerable to overfitting.

## Task 3 - Visualising observed and modelled GPP for KNN and Linear Regression

In this task the observed and modelled GPP for KNN and linear Regression are visualized with two charts. The charts cover all available data.

```{r warning=FALSE, message=FALSE}
library(ggplot2)
library(ggpubr)

# creating a dataframe with the dates and observed GPP
observed_GPP <- data.frame(Date = half_hourly_fluxes_fixed$TIMESTAMP, GPP = half_hourly_fluxes_fixed$GPP_NT_VUT_REF)

# adding columns with modelled GPP for linear regression and KNN
observed_GPP$lm <- predict(model_linear_regression, newdata = half_hourly_fluxes_fixed)
observed_GPP$knn <- predict(model_knn, newdata = half_hourly_fluxes_fixed)


# creating a line plot for observed and modelled GPP for linear regression
plot_linear_regression <- ggplot(observed_GPP, aes(x = Date)) +
  geom_line(aes(y = GPP, color = "Observed")) +
  geom_line(aes(y = lm, color = "Linear Regression")) +
  labs(x = "Date", y = "GPP", color = "Model") +
  ggtitle("Observed and Modelled GPP for Linear Regression")

# creating a line plot for observed and modelled GPP for KNN
plot_knn <- ggplot(observed_GPP, aes(x = Date)) +
  geom_line(aes(y = GPP, color = "Observed")) +
  geom_line(aes(y = knn, color = "KNN")) +
  labs(x = "Date", y = "GPP", color = "Model") +
  ggtitle("Observed and Modelled GPP for KNN")

# arranging the two plots in one plot (2 rows, one column)
ggarrange(plot_linear_regression, plot_knn, ncol = 1, nrow = 2)
```

## Task 4 - The role of k

With the knowledge gained from the previous chapter, a analysis of the role of k is made. For this a hypothesis is formulated on how the R2 and the MAE are effected by a k which is near 1 and a k which equals N.

### Task 4.1 - Creating a hypothesis

Based on the understanding of KNN i got of this lecture, i would suppose that if the k-value approaches to 1, the model has a lower variance but a higher bias, which results in a lower R2 value on the test and training models. The reason why this is the case, is that if k=1, the model will only use a single neighbor. Therefore the model is just dependent of this neighbor (lower variance because just focusing on one variable). This leads to overfitting of the training model which means it will not generalize well to new data. Therefore the R2 of the test model will decrease.

If the k-value approaches to N, i suppose the model will have a higher variance and a lower bias, because it focuses on all variable which are available. Therefore the model maybe generalize well to new data. Also the R2 value increases on the test model.

For the information i got from the MAE i suppose, that if the k-value approaches to 1, the MAE will be higher, because the model overfits. If the k-value approaches to N, the MAE will be lower on the test model, because the model maybe adapts well to new data.

Overall i think it is difficult to find the optimal k-value, because it depends on the dataset but also on the model created.

### Task 4.2 - Testing the hypothesis

Down below is a visualization which impelemnts the MAE to the model complexity (k value). So a "region" of underfitting can be seen at the point where the test error and the train error are both high. On the other hand a "region" of overfitting can be seen at the point where the test error starts to increase. The optimal model complexity is where the test error is lowest and the difference between test and train error is small. So with higher k value the model complexity increases to the point where the test error starts to increase and the model overfitts, whereas when the k-value is low the model underfitts (on the left side of the visualization), because test and train error are high.

```{r warning=FALSE}
library(rsample)
library(tidyr)
library(caret)
library(recipes)
library(ggplot2)

# testing the KNN model on different k values
k_values <- c(1:30)
mae_values <- sapply(k_values, function(k) get_mae(k)$mae_test)
mae_train_values <- sapply(k_values, function(k) get_mae(k)$mae_train)

# Visualize MAE as a function of k (to see model generalizability)
control_knn_mod <- data.frame(k = k_values, MAE_test = mae_values, MAE_train = mae_train_values)
ggplot(control_knn_mod, aes(x = k)) + 
  geom_line(aes(y = MAE_test, color = "Test")) + 
  geom_line(aes(y = MAE_train, color = "Train")) +
  geom_point(aes(y = MAE_test)) + 
  geom_point(aes(y = MAE_train)) + 
  xlab("k") + 
  ylab("MAE") +
  scale_color_manual(name = "Data set", values = c("red", "blue"))
```

### 4.3 - Finding the optimal k-value

```{r warning=FALSE}
# Finding the optimal k-value out of "k_values"
optimal_k <- k_values[which.min(mae_values)]

# Print optimal k value and MAE on test and training sets
cat("Optimal k value:", optimal_k, "\n")
cat("MAE on test set for optimal k value:", get_mae(optimal_k)$mae_test, "\n")
cat("MAE on training set for optimal k value:", get_mae(optimal_k)$mae_train, "\n")
```

