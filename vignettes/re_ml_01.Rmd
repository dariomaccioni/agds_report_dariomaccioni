---
title: "Report_5"
author: "Dario Maccioni"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: yes
    toc_float: yes
    numbered: yes
    theme: journal
---

# AGDS Report

## Report Exercise 5

In this Report Exercise implements a comparison of linear Regression and KNN models. Both were evaluated for a training set and a test set. The goal of this Report is to understand the observed differences between these two models. The Report is structured in 6 different Tasks.

### Task 1 - Data Visualization

```{r message = FALSE}
half_hourly_fluxes <- readr::read_csv("https://raw.githubusercontent.com/geco-bern/agds/main/data/df_for_stepwise_regression.csv") # Loading the dataset (.csv) into R and creating a table
```

As shown down below, approximately 5.7% of the whole data set are missing values. If one looks a the different predictors one sees that as a example the LW_IN_F_MDS Predictor has nearly 50% missing values. This is important for further analysis. 

```{r}
visdat::vis_miss(half_hourly_fluxes) # Checking if any data in the dataset is missing
```

```{r}
half_hourly_fluxes_fixed <- stats::na.omit(half_hourly_fluxes) # Deleting all rows with at least one value which equals "NA"
```

This chart is to show if there is any "bad data" in the data frame. As it can be seen, there is no long tail and the data looks ok. There is no obvious "bad data". 

```{r message = FALSE, warning=FALSE}
library(ggplot2)
half_hourly_fluxes |> 
  ggplot(aes(x = GPP_NT_VUT_REF, y = ..count..)) + 
  geom_histogram()
```

### Task 2 - Creating and visualising a linear regression model and a KNN model

```{r message = FALSE, warning=FALSE}
library(rsample)
library(tidyr)
library(caret)
library(recipes)

# splitting the data in a training (70%) and test set (30%)
set.seed(2000) # just for reproducibility reasons
split <- rsample::initial_split(half_hourly_fluxes_fixed, prop = 0.7, strata = "VPD_F")
half_hourly_fluxes_train <- rsample::training(split)
half_hourly_fluxes_test <- rsample::testing(split)

# pre_processing and ignoring the LW_IN_F variable because it has to much missing value and its probably not critcal for predicting GPP
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = half_hourly_fluxes_train |> drop_na()) |> 
  recipes::step_BoxCox(all_predictors()) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())

# making a linear Regression model
mod_lm <- caret::train(
  pp, 
  data = half_hourly_fluxes_train |> drop_na(), 
  method = "lm",
  trControl = caret::trainControl(method = "none"),
  metric = "RMSE"
)

# making a KNN model
mod_knn <- caret::train(
  pp, 
  data = half_hourly_fluxes_train, 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = data.frame(k = 8),
  metric = "RMSE"
)
```

#### Task 2.1 - Loading the Function created on a .R file

A folder named "Functions" was created to put large codes in it to simplify coding and the readflow. The code can be extract by the function source().

```{r message = FALSE, warning=FALSE}
source("../Functions/large_file.R")
```

#### Task 2.2 - Visualising the 2 models with the function created before

```{r warning=FALSE, message=FALSE}
# Linear Regression
eval_model(mod = mod_lm, df_train = half_hourly_fluxes_train, df_test = half_hourly_fluxes_test)
```

```{r warning=FALSE, message=FALSE}
# KNN
eval_model(mod = mod_knn, df_train = half_hourly_fluxes_train, df_test = half_hourly_fluxes_test)
```

#### 2.3 - Interpretation of the observed differences in the context of the bias-variance trade-off

__Why is the difference between the evaluation on the training and the test set larger for the KNN model than for the linear regression model?__

__Why is the does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model?__

__How would you position the KNN and the linear regression model along the spectrum of the bias-variance trade-off?__


### Task 5 - Visualising observed and modelled GPP for KNN and Linear Regressio

```{r}
library(ggplot2)

# creating a dataframe with the dates and observed GPP
observed <- data.frame(Date = half_hourly_fluxes_fixed$TIMESTAMP, GPP = half_hourly_fluxes_fixed$GPP_NT_VUT_REF)

# adding columns with modelled GPP for linear regression and KNN
observed$lm <- predict(mod_lm, newdata = half_hourly_fluxes_fixed)
observed$knn <- predict(mod_knn, newdata = half_hourly_fluxes_fixed)

# creating a line plot for observed and modelled GPP for linear regression
ggplot(observed, aes(x = Date)) +
  geom_line(aes(y = GPP, color = "Observed")) +
  geom_line(aes(y = lm, color = "Linear Regression")) +
  labs(x = "Date", y = "GPP", color = "Model") +
  ggtitle("Observed and Modelled GPP for Linear Regression")

# creating a line plot for observed and modelled GPP for KNN
ggplot(observed, aes(x = Date)) +
  geom_line(aes(y = GPP, color = "Observed")) +
  geom_line(aes(y = knn, color = "KNN")) +
  labs(x = "Date", y = "GPP", color = "Model") +
  ggtitle("Observed and Modelled GPP for KNN")

```

### Task 6 - the role of k

#### Task 6.1 - creating a hypothesis

Based on the understanding of KNN i got of this lecture, i would suppose that if the k-value approaches to 1, the model has a lower variance but a higher bias, which results in a lower R2 value on the test and training models. The reason why this is the case, is that if k=1, the model will only use a single neighbor. Therefore the model is just dependent of this neighbor (lower variance because just focusing on one variable). This leads to overfitting of the training model which means it will not generalize well to new data. Therefore the R2 of the test model will decrease.

If the k-value approaches to N, i suppose the model will have a higher variance and a lower bias, because it focuses on all variable which are available. Therefore the model will generalize well to new data. Also the R2 value increases on the test model.

For the information i got from the MAE i suppose, that if the k-value approaches to 1, the MAE will be higher, because the model overfits. If the k-value approaches to N, the MAE will be lower on the test model, because the model adapts well to new data.

Overall i think it is difficult to find the optimal k-value, because it depends on the dataset but also on the model created.

#### Task 6.2 - testing the hypothesis

#### function of the code

```{r}
library(rsample)
library(tidyr)
library(caret)
library(recipes)
library(ggplot2)

# Evaluate KNN model for different values of k
k_values <- c(1:30)
mae_values <- sapply(k_values, function(k) get_mae(k)$mae_test)
mae_train_values <- sapply(k_values, function(k) get_mae(k)$mae_train)

# Visualize MAE as a function of k
df <- data.frame(k = k_values, MAE_test = mae_values, MAE_train = mae_train_values)
ggplot(df, aes(x = k)) + 
  geom_line(aes(y = MAE_test, color = "Test")) + 
  geom_line(aes(y = MAE_train, color = "Train")) +
  geom_point(aes(y = MAE_test)) + 
  geom_point(aes(y = MAE_train)) + 
  xlab("k") + 
  ylab("MAE") +
  scale_color_manual(name = "Data set", values = c("red", "blue"))
```

#### Finding the opitmal k-value

```{r warning=FALSE}
# Find the optimal k
optimal_k <- k_values[which.min(mae_values)]

# Print optimal k value and MAE on test and training sets
cat("Optimal k value:", optimal_k, "\n")
cat("MAE on test set for optimal k value:", get_mae(optimal_k)$mae_test, "\n")
cat("MAE on training set for optimal k value:", get_mae(optimal_k)$mae_train, "\n")
```

